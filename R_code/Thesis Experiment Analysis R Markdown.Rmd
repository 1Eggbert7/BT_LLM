---
title: "Thesis Experiment Analysis"
author: "Alexander Leszczynski"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r intall packages}
#install.packages("patchwork")
#install# Load forcats library for reordering
# Install the psych package if not already installed
#install.packages("psych")

# Load the psych package


```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(Matrix)
library(lme4)
library(readxl)
library(ggplot2)
library(dplyr) # For data manipulation
library(scales) # For percentage labels
library(tidyr) # Needed for pivot_longer
library(moments) # For skewness and kurtosis
library(effsize) # For effect size calculations
library(rcompanion) # For 
library(lmerTest)
library(tidyverse)
library(patchwork)
library(forcats) # For reordering
library(psych) 
```

# Introduction

In this analysis, we examine the results of our experiment focusing on participants' preferred systems. The goal is to understand the distribution of preferences and gain insights that could inform future developments.

# Hypotheses

-   **H1**: The BT Framework will result in fewer task errors compared to the baseline when executing user requests.
-   **H2**: The BT Framework will generate a higher percentage of correct JSON sequences compared to the baseline.
-   **H3**: The BT Framework will be favored by participants over the baseline.
-   **H4**: The BT Framework will receive higher trust scores on the MDMT scale.
-   **H5**: The BT Framework will score higher on the RoSAS competence and warmth scales and lower on the discomfort scale than the baseline system.
-   **H6**: The BT Framework will achieve better results in explainability, as measured by the xAI scale.
-   **H7**: The BT Framework will have longer interaction time on average with participants than the Baseline.

### Reasons for the Hypotheses:

-   **H1**: The BT structure breaks down the decision-making process into smaller steps, leading to more systematic and predictable task execution, which should reduce the error rate compared to the baseline.
-   **H2**: When a new sequence needs to be generated, the BT framework specifically identifies this through a dedicated branch, allowing the LLM's prompt to focus solely on generating the correct JSON. Additionally, the BT framework includes an automated check that verifies the correctness of the JSON by evaluating functions, ingredients, and structure, ensuring higher accuracy. In contrast, the baseline system lacks this automated verification and relies solely on the long pre-prompt, which is less precise in guiding JSON generation.
-   **H3**: Participants are likely to favor systems that respond consistently and predictably, which the BT framework ensures more effectively than the baseline system.
-   **H4**: A structured system like the BT framework offers more understandable and consistent responses, which should increase users' trust compared to the less structured baseline system.
-   **H5**: The organized and deliberate responses of the BT framework enhance the perceived competence and warmth of the system while reducing user discomfort due to fewer unexpected outcomes.
-   **H6**: The step-by-step decision-making in the BT framework makes the system more explainable and transparent, leading to better results on the explainability (xAI) scale.
-   **H7**: The BT framework often involves more LLM requests running in the background, which can lead to longer response times as the system "thinks" more. Additionally, the BT framework is designed to prioritize clarification by asking more follow-up questions to ensure correct handling of the request, rather than quickly executing a task that could be incorrect. This structured approach to interaction naturally extends the overall interaction time compared to the baseline, which may prioritize speed over accuracy.

# Final Hypotheses

-   **H1**: The BT Framework will reduce task errors compared to the baseline system.
    -   *Reasoning*: The BT structure systematically breaks down the decision-making process into smaller, manageable steps, leading to more predictable and consistent task execution, which should reduce the overall error rate compared to the baseline.
-   **H2**: The BT Framework will score higher on the RoSAS scales for warmth and competence and lower on the discomfort scale compared to the baseline system.
    -   *Reasoning*: By offering structured and consistent responses, the BT Framework is expected to enhance the perceived competence and warmth of the system while minimizing user discomfort due to fewer unexpected or confusing outcomes.
-   **H3**: The BT Framework will achieve better results in both the MDMT (trust dimensions) and xAI (explainability) scales compared to the baseline.
    -   *Reasoning*: The BT framework’s structured approach not only improves transparency and explainability but also positively impacts participants' perceptions of the robot’s reliability, capability, and ethicality, as measured by the MDMT subscales. This holistic improvement supports both trust and explainability metrics.
-   **H4**: The BT Framework will be preferred by participants over the baseline system.
    -   *Reasoning*: Participants are likely to favor systems that demonstrate predictability and reliability, which the BT Framework is designed to achieve more effectively than the baseline. This preference is expected to be reflected in participants’ overall ratings and choices during the experiment.

# Initial Hypotheses

-   **H1**: The BT Framework will result in fewer task errors compared to the Baseline when executing user requests.
-   **H2**: The BT Framework was favored by the participants over the Baseline.
-   **H3**: The BT Framework will receive higher competence ratings on the RoSAS scale than the Baseline.
-   **H4**: The BT Framework will be perceived as warmer on the RoSAS warmth scale than the Baseline.
-   **H5**: Participants will report lower discomfort levels for the BT Framework than the Baseline on the RoSAS discomfort scale.
-   **H6**: The BT Framework will produce fewer errors in the generation of JSON sequences for high-level functions, as compared to the Baseline.
-   **H7**: The BT Framework will demonstrate better error recovery compared to the Baseline, though the frequency of these events may be limited based on the nature of the tasks.
-   **H8**: The BT Framework will receive higher reliability ratings on the MDMT Subscale than the Baseline.
-   **H9**: The BT Framework will receive higher capability ratings on the MDMT Subscale than the Baseline.
-   **H10**: The BT Framework will score higher in overall Capacity Trust (reliability + capability) on the MDMT scale than the Baseline.
-   **H11**: The BT Framework will receive higher ethical ratings on the MDMT Subscale than the Baseline.
-   **H12**: The BT Framework will receive higher sincerity ratings on the MDMT Subscale than the Baseline.
-   **H13**: The BT Framework will score higher in overall Moral Trust (ethical + sincere) on the MDMT scale than the Baseline.
-   **H14**: The BT Framework will score higher in the Explainability average score on the xAI scale than the Baseline.

### Objective Measures

-   **H1**, **H6**, **H7**

### Subjective Measures

-   **H2 - H5**, **H8 - H14**

# Overview

## Loading the Data

We begin by loading the necessary data from the Excel file.

```{r Load data}
# Read the Excel file
data <- read_excel("Experiment Results Overview.xlsx", sheet = "Overview Without dropouts")

# Remove rows where all values are NA
data <- data[rowSums(is.na(data)) != ncol(data), ]

# somehow row 42 even tho it contains nothing, still is in there
# Remove row 42 manually
data <- data[-42, ]

# show first few rows of the cleaned data
head(data)
```

### Exploring the Data

Before diving into the visualization, let's briefly explore the data to understand its structure.

-   **Total Participants:** `r nrow(data)`
-   **Columns Available:** `r paste(colnames(data), collapse = ", ")`

## Visualization

### Demographics

#### Gender

The following pie chart shows the distribution of gender among the participants.

```{r Gender Pie Chart}
# Calculate the frequency of each gender
gender_data <- data %>%
  count(Gender = `Gender`) %>%
  mutate(Percentage = n / sum(n) * 100)

# Create a pie chart for gender distribution with custom colors
ggplot(gender_data, aes(x = "", y = Percentage, fill = Gender)) +
  geom_bar(stat = "identity", width = 1, color = "white") +
  coord_polar(theta = "y") +
  labs(title = "Gender Distribution",
       x = NULL,
       y = NULL,
       fill = "Gender") +
  theme_void() +
  geom_text(aes(label = paste0(round(Percentage, 1), "%")), 
            position = position_stack(vjust = 0.5)) +
  scale_fill_manual(values = c("Female" = "#33adbb", "Male" = "#fd7e23", "Non-binary" = "#34495E", "Prefer not to say" = "#7D3C98")) # Customize with darker blue and red
```

#### Age Distribution

The following histogram displays the distribution of ages among the participants. The red dashed line indicates the average age of the participants.

-   **Average Age:** `r round(average_age, 1)`

```{r age}
# Calculate the average age
average_age <- mean(data$Age, na.rm = TRUE)

# Create a histogram for age distribution with thinner columns
ggplot(data, aes(x = Age)) +
  geom_histogram(binwidth = 1, fill = "#3366CC", color = "white", alpha = 0.7) + # Changed binwidth to 1
  labs(title = "Age Distribution of Participants",
       x = "Age",
       y = "Frequency") +
  theme_minimal() +
  geom_vline(aes(xintercept = average_age), color = "red", linetype = "dashed", size = 1) +
  annotate("text", x = average_age + 1, y = max(table(data$Age)), 
           label = paste("Average Age:", round(average_age, 1)), color = "black", vjust = 1, hjust = 0)

```

### Preferred System Distribution

The following pie chart displays the distribution of participants' preferred systems.

```{r Preferred System}
# Calculate the frequency of each preferred system
preferred_data <- data %>%
  count(Preferred_System = `Preferred System`) %>%
  mutate(Percentage = n / sum(n) * 100)

# Create a pie chart for preferred system distribution
ggplot(preferred_data, aes(x = "", y = Percentage, fill = Preferred_System)) +
  geom_bar(stat = "identity", width = 1, color = "white") +
  coord_polar(theta = "y") +
  labs(title = "Distribution of Preferred Systems",
       x = NULL,
       y = NULL,
       fill = "Preferred System") +
  theme_void() +
  geom_text(aes(label = paste0(round(Percentage, 1), "%")), 
            position = position_stack(vjust = 0.5))+
  scale_fill_manual(values = c("BT" = "#11B54E", "Base" = "#0C83C4")) # Customize with darker blue and red

```

# Survey Analysis

```{r Load survey data}
# Read the Excel file sheet with the Baseline Results
survey_data_Base <- read_excel("Experiment Results Overview.xlsx", sheet = 5)

# Read the Excel file sheet with BT results 
survey_data_BT <- read_excel("Experiment Results Overview.xlsx", sheet = "BT Results Without dropouts")

                           
# Display the first few rows of the data
head(survey_data_Base)
head(survey_data_BT)
```

## Warmth, Competence, and Discomfort Comparison

In this section, we will compare the average ratings for **Warmth**, **Competence**, and **Discomfort** between the baseline survey (`survey_data_Base`) and the behavior tree survey (`survey_data_BT`).

### Data Preparation

To facilitate this comparison, we will extract the relevant columns from both surveys and merge them into a single data frame for easier visualization.

```{r Data Preparation}
# Select relevant columns
warmth_comp_discomfort <- data.frame(
  Measure = c("Warmth (Average)", "Competence (Average)", "Discomfort (Average)"),
  Base = c(mean(survey_data_Base$`Warmth (Average)`, na.rm = TRUE),
           mean(survey_data_Base$`Competence (Average)`, na.rm = TRUE),
           mean(survey_data_Base$`Discomfort (Average)`, na.rm = TRUE)),
  BT = c(mean(survey_data_BT$`Warmth (Average)`, na.rm = TRUE),
         mean(survey_data_BT$`Competence (Average)`, na.rm = TRUE),
         mean(survey_data_BT$`Discomfort (Average)`, na.rm = TRUE))
)


# Display the data
warmth_comp_discomfort
```

```{r visualize data}
# Reshape data for visualization
warmth_comp_discomfort_long <- warmth_comp_discomfort %>%
  pivot_longer(cols = c(Base, BT), names_to = "Survey_Type", values_to = "Average")

# Create the bar chart
ggplot(warmth_comp_discomfort_long, aes(x = Measure, y = Average, fill = Survey_Type)) +
  geom_bar(stat = "identity", position = position_dodge(), color = "white") +
  labs(title = "Comparison of Warmth, Competence, and Discomfort (Average)",
       x = "Measure",
       y = "Average Rating",
       fill = "Survey Type") +
  theme_minimal() +
  scale_fill_manual(values = c("Base" = "#0C83C4", "BT" = "#11B54E")) + # Customizing with the preferred darker blue and red
  geom_text(aes(label = round(Average, 2)), 
            position = position_dodge(width = 0.9), vjust = -0.25, size = 3.5)


```

### Competence

## Let's take a closer look at the competence distribution between the two

```{r deep_dive_competence, echo=TRUE}
# Remove NA values from the "Competence (Average)" columns in both datasets
competence_BT <- na.omit(survey_data_BT$`Competence (Average)`)
competence_Base <- na.omit(survey_data_Base$`Competence (Average)`)

# Check the lengths to ensure they are both 39
print(paste("Number of non-NA measures in BT:", length(competence_BT)))
print(paste("Number of non-NA measures in Base:", length(competence_Base)))

# Create the combined data frame
competence_data <- data.frame(
  ID = rep(1:length(competence_BT), 2), 
  Competence = c(competence_BT, competence_Base),
  Survey_Type = rep(c("BT", "Base"), each = length(competence_BT))
)


# Remove any NA values in case there are missing measures
competence_data <- na.omit(competence_data)
```

```{r more plotting}
# Calculate the mean and standard deviation for each Survey_Type
stats_summary <- competence_data %>%
  group_by(Survey_Type) %>%
  summarise(Mean = mean(Competence), SD = sd(Competence))

# Create the density plot with annotations for standard deviations and means
ggplot(competence_data, aes(x = Competence, fill = Survey_Type)) +
  geom_density(alpha = 0.4) +  # Create the density plot with transparency for overlap
  # Add normal distribution line for the Base survey type only in the Base plot
  stat_function(data = subset(stats_summary, Survey_Type == "Base"), 
                fun = dnorm, 
                args = list(mean = stats_summary$Mean[stats_summary$Survey_Type == "Base"], 
                            sd = stats_summary$SD[stats_summary$Survey_Type == "Base"]), 
                color = "black", size = 1, linetype = "dashed", inherit.aes = FALSE) +
  # Add normal distribution line for the BT survey type only in the BT plot
  stat_function(data = subset(stats_summary, Survey_Type == "BT"), 
                fun = dnorm, 
                args = list(mean = stats_summary$Mean[stats_summary$Survey_Type == "BT"], 
                            sd = stats_summary$SD[stats_summary$Survey_Type == "BT"]), 
                color = "black", size = 1, linetype = "dashed", inherit.aes = FALSE) +
  geom_vline(data = stats_summary,
             aes(xintercept = Mean),
             linetype = "dashed", size = 1) +
  # Add shaded regions to indicate 1 SD from the mean for each Survey_Type
  geom_rect(data = stats_summary,
            aes(xmin = Mean - SD, xmax = Mean + SD, ymin = 0, ymax = Inf, fill = Survey_Type),
            alpha = 0.2, inherit.aes = FALSE) +
  labs(title = "Competence Rating Distribution with Standard Deviation and Mean",
       x = "Competence Rating",
       y = "Density") +
  theme_minimal() +
  scale_fill_manual(values = c("Base" = "#3366CC", "BT" = "#11B54E")) +
  scale_color_manual(values = c("Base" = "#3366CC", "BT" = "#11B54E")) +
  theme(axis.text.x = element_text(angle = 0, hjust = 0.5)) +
  facet_wrap(~ Survey_Type, ncol = 1) +
  # Annotate the standard deviations for the Base plot
  geom_text(data = subset(stats_summary, Survey_Type == "Base"),
            aes(x = 7.5, y = 0.30, label = paste("SD (Base):", round(SD, 2))),
            color = "black", size = 4, hjust = 1, inherit.aes = FALSE) +
  # Annotate the standard deviations for the BT plot
  geom_text(data = subset(stats_summary, Survey_Type == "BT"),
            aes(x = 7.5, y = 0.30, label = paste("SD (BT):", round(SD, 2))),
            color = "black", size = 4, hjust = 1, inherit.aes = FALSE) +
  # Annotate the means for the Base plot
  geom_text(data = subset(stats_summary, Survey_Type == "Base"),
            aes(x = 7.5, y = 0.35, label = paste("Mean (Base):", round(Mean, 2))),
            color = "black", size = 4, hjust = 1, inherit.aes = FALSE) +
  # Annotate the means for the BT plot
  geom_text(data = subset(stats_summary, Survey_Type == "BT"),
            aes(x = 7.5, y = 0.35, label = paste("Mean (BT):", round(Mean, 2))),
            color = "black", size = 4, hjust = 1, inherit.aes = FALSE)


# Bin the Competence values into intervals of 1 for simpler visualization
competence_data$Competence_Binned <- cut(competence_data$Competence, breaks = seq(0, 7, by = 0.5))

# Count the occurrences in each bin for each survey type
competence_summary_binned <- competence_data %>%
  group_by(Competence_Binned, Survey_Type) %>%
  summarise(Count = n(), .groups = "drop")

# Create a faceted bar plot with binned data
ggplot(competence_summary_binned, aes(x = Competence_Binned, y = Count, fill = Survey_Type)) +
  geom_bar(stat = "identity", color = "white") +
  labs(title = "Binned Competence Rating Distribution",
       x = "Competence Rating (Binned)",
       y = "Quantity of Ratings") +
  theme_minimal() +
  scale_fill_manual(values = c("Base" = "#3366CC", "BT" = "#11B54E")) +
  theme(axis.text.x = element_text(angle = 0, hjust = 0.5)) +
  facet_wrap(~ Survey_Type, ncol = 1)

```

```{r moar shenanigans}
# Perform Shapiro-Wilk test for the Base competence ratings
base_data <- competence_data %>% filter(Survey_Type == "Base") %>% pull(Competence)
shapiro_base <- shapiro.test(base_data)

# Perform Shapiro-Wilk test for the BT competence ratings
bt_data <- competence_data %>% filter(Survey_Type == "BT") %>% pull(Competence)
shapiro_bt <- shapiro.test(bt_data)

# Print the results
cat("Shapiro-Wilk Test for Base Competence Ratings:\n")
cat("W =", shapiro_base$statistic, ", p-value =", shapiro_base$p.value, "\n\n")

cat("Shapiro-Wilk Test for BT Competence Ratings:\n")
cat("W =", shapiro_bt$statistic, ", p-value =", shapiro_bt$p.value, "\n")


### notes
#aov()
#amod <- lmer(yield ~ treat + (1|blend), penicillin, REML=FALSE)
# (1|blend)
#lm()
#model1 <-aov(closeness ~ gender + bc_freq, data = questionnairesd)
#Anova(model1, type="III")

```

According to the Shapiro-Wilk test, both the Baseline and BT Competence ratings are normally distributed. The p-values for both tests are greater than 0.05, indicating that we do not have enough evidence to reject the null hypothesis of normality.

```{r mixed effects competence}

# Fit the mixed model
model <- lmer(Competence ~ Survey_Type + (1 | ID), data = competence_data)

# Summarize the model
summary(model)


```

## Mixed-Effects Model Summary

### Model Overview

-   **Formula**: `Competence ~ Survey_Type + (1 | ID)`
-   **Purpose**: To compare competence ratings between "BT" and "Base" conditions while accounting for participant variability, taking random effects into account.

### Key Results

1.  **Random Effects (Participant Variability)**:
    -   **Variance (ID)**: `0.6012`, Std. Dev = `0.7754`
        -   Indicates that individual participants' baseline ratings vary around the overall mean, reflecting moderate variability in ratings across participants.
    -   **Residual Variance**: `0.5228`, Std. Dev = `0.7231`
        -   Represents within-participant variability in ratings.
2.  **Fixed Effects (Survey Type Differences)**:
    -   **Intercept (Base condition)**:
        -   Estimate = `4.7561`, t-value = `28.72`
        -   Represents the average competence rating for the "Base" condition.
    -   **Survey_TypeBT**:
        -   Estimate = `0.2683`, t-value = `1.68`
        -   Suggests a slight increase in competence ratings for the "BT" condition, though not statistically significant.
3.  **Interpretation**:
    -   There is a marginal effect suggesting that the "BT" condition may be rated higher in competence than the "Base" condition.
    -   Random effects show participant variability, making `ID` a suitable random effect to capture individual differences in ratings.

### Conclusion

-   The model indicates moderate variability across participants and a slight, non-significant trend for higher competence ratings in the "BT" condition compared to the "Base" condition.

# Results and Analysis

## H1:

-   **H1**: The BT Framework will reduce task errors compared to the baseline system.
    -   *Reasoning*: The BT structure systematically breaks down the decision-making process into smaller, manageable steps, leading to more predictable and consistent

### Visualization and Analysis

```{r H1 visuals}
# Calculate the mean and standard deviation for Baseline and BT mistakes
mistakes_stats <- data %>%
  summarise(
    Mean_Baseline_Mistakes = mean(`Baseline Mistakes`),
    SD_Baseline_Mistakes = sd(`Baseline Mistakes`),
    Mean_BT_Mistakes = mean(`BT Mistakes`),
    SD_BT_Mistakes = sd(`BT Mistakes`),
    Total_Baseline_Mistakes = sum(`Baseline Mistakes`),
    Total_BT_Mistakes = sum(`BT Mistakes`)
  )



# Convert the data into a format compatible with a pie chart
mistakes_melted <- data.frame(
  System = c("Baseline", "BT"),
  Total_Mistakes = c(mistakes_stats$Total_Baseline_Mistakes, mistakes_stats$Total_BT_Mistakes)
)

# Calculate the percentages for each system
mistakes_melted$Percentage <- mistakes_melted$Total_Mistakes / sum(mistakes_melted$Total_Mistakes)

# Create the pie chart with percentages and larger legend text
ggplot(mistakes_melted, aes(x = "", y = Total_Mistakes, fill = System)) +
  geom_bar(stat = "identity", width = 1, color = 'white') +
  coord_polar("y", start = 0) +
  geom_text(aes(label = scales::percent(Percentage, accuracy = 1)), 
            position = position_stack(vjust = 0.5), size = 5, color = "black") +
  labs(title = "Total Mistakes Comparison: Baseline vs. BT") +
  theme_void() +
  theme(
    plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
    legend.text = element_text(size = 12) # Adjust font size of the legend text
  ) +
  scale_fill_manual(values = c("Baseline" = "#3366CC", "BT" = "#11B54E")) 




# Calculate the mean and standard deviation for Baseline and BT mistakes
mistakes_stats <- data %>%
  summarise(
    Mean_Baseline_Mistakes = mean(`Baseline Mistakes`),
    SD_Baseline_Mistakes = sd(`Baseline Mistakes`),
    Mean_BT_Mistakes = mean(`BT Mistakes`),
    SD_BT_Mistakes = sd(`BT Mistakes`)
  )

# Convert the data to long format for violin plot
long_data <- data %>%
  pivot_longer(cols = c(`Baseline Mistakes`, `BT Mistakes`), 
               names_to = "System", 
               values_to = "Mistakes") %>%
  mutate(System = ifelse(System == "Baseline Mistakes", "Baseline", "BT"))

# Create a dataframe for annotation
annotation_data <- data.frame(
  System = c("Baseline", "BT"),
  x_pos = c(1, 2),
  Mean = c(mistakes_stats$Mean_Baseline_Mistakes, mistakes_stats$Mean_BT_Mistakes),
  SD = c(mistakes_stats$SD_Baseline_Mistakes, mistakes_stats$SD_BT_Mistakes)
)

# Create the violin plot with increased font sizes
ggplot(long_data, aes(x = System, y = Mistakes, fill = System)) +
  geom_violin(trim = TRUE, alpha = 0.6) +
  geom_boxplot(width = 0.1, outlier.shape = NA, color = "black", alpha = 0.5) +
  stat_summary(fun = mean, geom = "point", shape = 20, size = 3, color = "black") +
  labs(title = "Violin Plot of Mistakes: Baseline vs. BT",
       x = "System",
       y = "Number of Mistakes") +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),  # Title font size
    axis.title.x = element_text(size = 14),  # X-axis label font size
    axis.title.y = element_text(size = 14),  # Y-axis label font size
    axis.text = element_text(size = 12),  # Axis tick labels font size
    legend.title = element_text(size = 14),  # Legend title font size
    legend.text = element_text(size = 12)   # Legend text font size
  ) +
  scale_fill_manual(values = c("Baseline" = "#3366CC", "BT" = "#11B54E")) +
  coord_cartesian(ylim = c(0, 4)) +  # Adjust y-axis limits to prevent negative values
  # Annotate mean and SD for each system
  geom_text(data = annotation_data, aes(x = x_pos, y = 3.5, 
                label = paste("Mean:", round(Mean, 2), "\nSD:", round(SD, 2))),
            color = "black", size = 5, hjust = 0.5)  # Increase annotation font size



```

1.  **Total Mistakes Comparison: Baseline vs. BT (Pie Chart)**:
    -   The pie chart illustrates the proportion of total mistakes made in the Baseline and BT systems.
    -   The **Baseline** system accounts for **69%** of the total mistakes, while the **BT** system accounts for only **31%**.
    -   This indicates that the BT Framework significantly reduces the overall number of mistakes compared to the baseline system, supporting the hypothesis (H1) that the BT system improves performance.
2.  **Violin Plot of Mistakes: Baseline vs. BT**:
    -   The violin plot shows the distribution of mistakes for both the Baseline and BT systems. It includes:
        -   The **shape** of the distribution, representing the density of mistakes across participants.
        -   The **boxplot** within the violin, which shows the median (thick line) and the interquartile range (IQR).
        -   The **mean** (black dot) for each system.
    -   **Interpretation**:
        -   The **Baseline** system has a mean of **1.17** mistakes with a standard deviation of **0.95**, indicating a wider spread of mistakes among participants.
        -   The **BT** system, on the other hand, has a lower mean of **0.54** mistakes and a smaller standard deviation of **0.71**, suggesting that it not only has fewer mistakes on average but also displays more consistent performance across participants.
    -   The violin plot clearly supports the notion that the **BT Framework** reduces both the frequency and variability of mistakes compared to the baseline system.

#### Conclusion of Visualization

These visualizations provide strong evidence in support of **H1**. The BT Framework not only reduces the average number of task errors but also ensures more consistent performance across participants compared to the baseline system. This supports the hypothesis that the BT Framework's structured approach effectively minimizes task errors.

### Statistical Analysis for mistakes made in Baseline and BT

```{r H1 stats}

# Shapiro-Wilk Test for Baseline Mistakes
shapiro_baseline <- shapiro.test(data$`Baseline Mistakes`)
# Shapiro-Wilk Test for BT Mistakes
shapiro_bt <- shapiro.test(data$`BT Mistakes`)

# Output the results
shapiro_baseline
shapiro_bt

# Calculate skewness and kurtosis for Baseline Mistakes
baseline_skewness <- skewness(data$`Baseline Mistakes`)
baseline_kurtosis <- kurtosis(data$`Baseline Mistakes`)

# Calculate skewness and kurtosis for BT Mistakes
bt_skewness <- skewness(data$`BT Mistakes`)
bt_kurtosis <- kurtosis(data$`BT Mistakes`)

# Print the results
print(paste("Baseline skewness:", baseline_skewness))
print(paste("Baseline kurtosis:", baseline_kurtosis))
print(paste("BT skewness:", bt_skewness))
print(paste("BT kurtosis:", bt_kurtosis))

ggplot(data, aes(x = `Baseline Mistakes`)) +
  geom_histogram(binwidth = 1, color = "black", fill = "#3366CC") +
  labs(title = "Histogram of Baseline Mistakes", x = "Baseline Mistakes", y = "Count")

ggplot(data, aes(x = `BT Mistakes`)) +
  geom_histogram(binwidth = 1, color = "black", fill = "#11B54E") +
  labs(title = "Histogram of BT Mistakes", x = "BT Mistakes", y = "Count")




```

```{r}

wilcox_result <- wilcox.test(data$`Baseline Mistakes`, data$`BT Mistakes`, 
                             paired = FALSE, 
                             alternative = "two.sided")

wilcox.test(data$`Baseline Mistakes`, data$`BT Mistakes`, 
            alternative = "two.sided")

wilcox.test(data$`Baseline Mistakes`, data$`BT Mistakes`, 
            paired = TRUE, 
            alternative = "two.sided")

ks.test(data$`Baseline Mistakes`, data$`BT Mistakes`)

cliff.delta(data$`Baseline Mistakes`, data$`BT Mistakes`)

```

#### Results

The statistical analysis was conducted to compare the number of mistakes between the Baseline and BT systems. The following tests were performed:

1.  **Wilcoxon Rank Sum Test (Mann-Whitney U Test)**:
    -   **W = 1166.5**, **p-value = 0.001222** (approximate due to ties)
    -   The p-value is less than 0.05, indicating a **statistically significant difference** between the distributions of Baseline and BT mistakes.
    -   This result suggests that the BT system has a significantly different number of mistakes compared to the Baseline system.
2.  **Wilcoxon Signed Rank Test**:
    -   **V = 331**, **p-value = 0.002519** (approximate due to ties and zero values)
    -   The p-value is less than 0.05, indicating a **statistically significant difference** when comparing paired observations between the Baseline and BT systems.
    -   This supports the conclusion that the BT system has a different (likely lower) number of mistakes compared to the Baseline, assuming that each observation in the Baseline system is paired with a corresponding observation in the BT system.
3.  **Kolmogorov-Smirnov Test**:
    -   **D = 0.29268**, **p-value = 0.01372**
    -   The p-value is less than 0.05, indicating that the distributions of the Baseline and BT mistakes are significantly different.
    -   This test highlights that the Baseline and BT systems have different error distributions in terms of location, spread, and skewness.
4.  **Cliff's Delta**:
    -   **Delta Estimate**: **0.3878644** (medium effect)
    -   **95% Confidence Interval**: [0.1564692, 0.5788971]
    -   Cliff's Delta measures the magnitude of the difference between the two groups. A value of **0.3878644** indicates a **medium effect**, suggesting that the difference between the Baseline and BT systems is not only statistically significant but also **meaningful** in practice.
    -   The confidence interval does not include 0, further supporting the presence of a medium effect size.

**Summary**:\
All tests indicate a **statistically significant difference** between the Baseline and BT systems, with the BT system showing fewer mistakes. The Wilcoxon tests confirm the difference in central tendency, while the Kolmogorov-Smirnov test highlights differences in the overall distribution shape. Cliff’s Delta suggests a **medium effect size**, demonstrating that the reduction in mistakes with the BT system is practically meaningful, not just statistically significant. Note that the p-values reported for the Wilcoxon tests are approximations due to ties and zero values in the data. These results strongly support the hypothesis (**H1**) that the BT Framework reduces task errors compared to the baseline system.

## H2: RoSAS Scales for Warmth, Competence, and Discomfort

-   **H2**: The BT Framework will score higher on warmth and competence and lower on discomfort compared to the baseline system.
    -   *Reasoning*: The structured approach of the BT Framework is expected to improve user perceptions on warmth, competence, and reduce discomfort.

### Visualization and Analysis

#### Data Preparation

```{r Data Preparation for RoSAS}
# Extract average ratings for Warmth, Competence, and Discomfort from both surveys
warmth_comp_discomfort <- data.frame(
  Measure = c("Warmth (Average)", "Competence (Average)", "Discomfort (Average)"),
  Base = c(mean(survey_data_Base$`Warmth (Average)`, na.rm = TRUE),
           mean(survey_data_Base$`Competence (Average)`, na.rm = TRUE),
           mean(survey_data_Base$`Discomfort (Average)`, na.rm = TRUE)),
  BT = c(mean(survey_data_BT$`Warmth (Average)`, na.rm = TRUE),
         mean(survey_data_BT$`Competence (Average)`, na.rm = TRUE),
         mean(survey_data_BT$`Discomfort (Average)`, na.rm = TRUE))
)

# Display the prepared data
warmth_comp_discomfort
```

```{r Visualization of RoSAS Measures}
# Reshape data for visualization
warmth_comp_discomfort_long <- warmth_comp_discomfort %>%
  pivot_longer(cols = c(Base, BT), names_to = "Survey_Type", values_to = "Average")

# Create the bar chart
ggplot(warmth_comp_discomfort_long, aes(x = Measure, y = Average, fill = Survey_Type)) +
  geom_bar(stat = "identity", position = position_dodge(), color = "white") +
  labs(title = "Comparison of Warmth, Competence, and Discomfort (Average)",
       x = "Measure",
       y = "Average Rating",
       fill = "Survey Type") +
  theme_minimal() +
  scale_fill_manual(values = c("Base" = "#0C83C4", "BT" = "#11B54E")) +
  geom_text(aes(label = round(Average, 2)), 
            position = position_dodge(width = 0.9), vjust = -0.25, size = 3.5)

```

***Average Comparison of Warmth, Competence, and Discomfort (Bar Chart):*** The bar chart visualizes average ratings for warmth, competence, and discomfort. Observations: - The BT system shows increased warmth and competence and reduced discomfort ratings compared to the baseline.

```{r Descriptive Statistics for RoSAS Measures}
# Calculate mean and SD for each measure in Base and BT
warmth_mean_base <- mean(survey_data_Base$`Warmth (Average)`, na.rm = TRUE)
warmth_sd_base <- sd(survey_data_Base$`Warmth (Average)`, na.rm = TRUE)
competence_mean_base <- mean(survey_data_Base$`Competence (Average)`, na.rm = TRUE)
competence_sd_base <- sd(survey_data_Base$`Competence (Average)`, na.rm = TRUE)
discomfort_mean_base <- mean(survey_data_Base$`Discomfort (Average)`, na.rm = TRUE)
discomfort_sd_base <- sd(survey_data_Base$`Discomfort (Average)`, na.rm = TRUE)

warmth_mean_bt <- mean(survey_data_BT$`Warmth (Average)`, na.rm = TRUE)
warmth_sd_bt <- sd(survey_data_BT$`Warmth (Average)`, na.rm = TRUE)
competence_mean_bt <- mean(survey_data_BT$`Competence (Average)`, na.rm = TRUE)
competence_sd_bt <- sd(survey_data_BT$`Competence (Average)`, na.rm = TRUE)
discomfort_mean_bt <- mean(survey_data_BT$`Discomfort (Average)`, na.rm = TRUE)
discomfort_sd_bt <- sd(survey_data_BT$`Discomfort (Average)`, na.rm = TRUE)

# Display results
data.frame(
  Measure = c("Warmth", "Competence", "Discomfort"),
  Mean_Base = c(warmth_mean_base, competence_mean_base, discomfort_mean_base),
  SD_Base = c(warmth_sd_base, competence_sd_base, discomfort_sd_base),
  Mean_BT = c(warmth_mean_bt, competence_mean_bt, discomfort_mean_bt),
  SD_BT = c(warmth_sd_bt, competence_sd_bt, discomfort_sd_bt)
)

```

**Overview**: The following statistics compare the baseline and BT framework on warmth, competence, and discomfort.

**Warmth** Baseline Mean: 3.50, SD: 1.28 BT Mean: 3.70, SD: 1.12 Interpretation: The BT framework has a slightly higher warmth score, with a lower standard deviation, indicating more consistent ratings across participants.

**Competence** Baseline Mean: 4.76, SD: 1.11 BT Mean: 5.02, SD: 1.01 Interpretation: Competence ratings are higher on average in the BT framework, with a smaller standard deviation, suggesting participants found the BT framework more competent and rated it more consistently.

**Discomfort** Baseline Mean: 2.21, SD: 0.99 BT Mean: 1.91, SD: 0.86 Interpretation: Discomfort scores are lower with the BT framework, which aligns with the hypothesis that a structured approach reduces discomfort.

### Statistical Analysis

```{r Normality Test}
# Shapiro-Wilk test for Warmth in Base and BT
shapiro_warmth_base <- shapiro.test(survey_data_Base$`Warmth (Average)`)
shapiro_warmth_bt <- shapiro.test(survey_data_BT$`Warmth (Average)`)

# Repeat for Competence and Discomfort
shapiro_competence_base <- shapiro.test(survey_data_Base$`Competence (Average)`)
shapiro_competence_bt <- shapiro.test(survey_data_BT$`Competence (Average)`)
shapiro_discomfort_base <- shapiro.test(survey_data_Base$`Discomfort (Average)`)
shapiro_discomfort_bt <- shapiro.test(survey_data_BT$`Discomfort (Average)`)

# Output the Shapiro-Wilk results
list(
  Warmth_Base = shapiro_warmth_base,
  Warmth_BT = shapiro_warmth_bt,
  Competence_Base = shapiro_competence_base,
  Competence_BT = shapiro_competence_bt,
  Discomfort_Base = shapiro_discomfort_base,
  Discomfort_BT = shapiro_discomfort_bt
)

```

#### Normality Test Results

**Purpose**: Shapiro-Wilk normality tests were conducted to check if each distribution is normally distributed.

**Warmth** Baseline: W=0.94, p = 0.031 (not normally distributed) BT: W=0.98, p = 0.728 (normally distributed)

**Competence** Baseline: W=0.96, p = 0.108 (normally distributed) BT: W=0.95, p = 0.080 (normally distributed)

**Discomfort** Baseline: W=0.91, p = 0.004 (not normally distributed) BT: W=0.87, p = 0.0002 (not normally distributed)

**Interpretation**: Only competence ratings are normally distributed for both Baseline and BT conditions. Since warmth and discomfort ratings have non-normal distributions, non-parametric tests may be more appropriate for further analysis of these measures.

Since only the Competence ratings have a normal distribution according to the Shapiro-Wilk tests, I am using a mixed effect model for Competence and for Warmth and Discomfort the Wilcoxon Signed Rank test will be used for its paired data

```{r more visuals plotting}
# Calculate the mean and standard deviation for each Survey_Type
stats_summary <- competence_data %>%
  group_by(Survey_Type) %>%
  summarise(Mean = mean(Competence), SD = sd(Competence))

# Create the density plot with annotations for standard deviations and means
ggplot(competence_data, aes(x = Competence, fill = Survey_Type)) +
  geom_density(alpha = 0.4) +  # Create the density plot with transparency for overlap
  # Add normal distribution line for the Base survey type only in the Base plot
  stat_function(data = subset(stats_summary, Survey_Type == "Base"), 
                fun = dnorm, 
                args = list(mean = stats_summary$Mean[stats_summary$Survey_Type == "Base"], 
                            sd = stats_summary$SD[stats_summary$Survey_Type == "Base"]), 
                color = "black", size = 1, linetype = "dashed", inherit.aes = FALSE) +
  # Add normal distribution line for the BT survey type only in the BT plot
  stat_function(data = subset(stats_summary, Survey_Type == "BT"), 
                fun = dnorm, 
                args = list(mean = stats_summary$Mean[stats_summary$Survey_Type == "BT"], 
                            sd = stats_summary$SD[stats_summary$Survey_Type == "BT"]), 
                color = "black", size = 1, linetype = "dashed", inherit.aes = FALSE) +
  geom_vline(data = stats_summary,
             aes(xintercept = Mean),
             linetype = "dashed", size = 1) +
  # Add shaded regions to indicate 1 SD from the mean for each Survey_Type
  geom_rect(data = stats_summary,
            aes(xmin = Mean - SD, xmax = Mean + SD, ymin = 0, ymax = Inf, fill = Survey_Type),
            alpha = 0.2, inherit.aes = FALSE) +
  labs(title = "Competence Rating Distribution with Standard Deviation and Mean",
       x = "Competence Rating",
       y = "Density") +
  theme_minimal() +
  scale_fill_manual(values = c("Base" = "#3366CC", "BT" = "#11B54E")) +
  scale_color_manual(values = c("Base" = "#3366CC", "BT" = "#11B54E")) +
  theme(axis.text.x = element_text(angle = 0, hjust = 0.5)) +
  facet_wrap(~ Survey_Type, ncol = 1) +
  # Annotate the standard deviations for the Base plot
  geom_text(data = subset(stats_summary, Survey_Type == "Base"),
            aes(x = 7.5, y = 0.30, label = paste("SD (Base):", round(SD, 2))),
            color = "black", size = 4, hjust = 1, inherit.aes = FALSE) +
  # Annotate the standard deviations for the BT plot
  geom_text(data = subset(stats_summary, Survey_Type == "BT"),
            aes(x = 7.5, y = 0.30, label = paste("SD (BT):", round(SD, 2))),
            color = "black", size = 4, hjust = 1, inherit.aes = FALSE) +
  # Annotate the means for the Base plot
  geom_text(data = subset(stats_summary, Survey_Type == "Base"),
            aes(x = 7.5, y = 0.35, label = paste("Mean (Base):", round(Mean, 2))),
            color = "black", size = 4, hjust = 1, inherit.aes = FALSE) +
  # Annotate the means for the BT plot
  geom_text(data = subset(stats_summary, Survey_Type == "BT"),
            aes(x = 7.5, y = 0.35, label = paste("Mean (BT):", round(Mean, 2))),
            color = "black", size = 4, hjust = 1, inherit.aes = FALSE)

```

```{r sht}
# Prepare data for mixed-effects model
rosas_data <- data.frame(
  ID = rep(1:length(competence_BT), 3),
  Rating = c(survey_data_Base$`Warmth (Average)`, survey_data_BT$`Warmth (Average)`, 
             survey_data_Base$`Competence (Average)`, survey_data_BT$`Competence (Average)`, 
             survey_data_Base$`Discomfort (Average)`, survey_data_BT$`Discomfort (Average)`),
  Measure = rep(c("Warmth", "Competence", "Discomfort"), each = length(competence_BT) * 2),
  Survey_Type = rep(c("Base", "BT"), each = length(competence_BT))
)

# Recalculate stats if needed
stats_summary <- rosas_data %>%
  group_by(Measure, Survey_Type) %>%
  summarise(Mean = mean(Rating, na.rm = TRUE), SD = sd(Rating, na.rm = TRUE), .groups = "drop")

# Create density plots for each measure
ggplot(rosas_data, aes(x = Rating, fill = Survey_Type)) +
  geom_density(alpha = 0.4) +
  geom_vline(data = stats_summary, aes(xintercept = Mean, color = Survey_Type), linetype = "dashed", size = 1) +
  facet_wrap(~ Measure, scales = "free", ncol = 1) +
  labs(title = "RoSAS Ratings Distribution: Competence, Discomfort, and Warmth",
       x = "Rating", y = "Density") +
  theme_minimal() +
  scale_fill_manual(values = c("Base" = "#3366CC", "BT" = "#11B54E")) +
  scale_color_manual(values = c("Base" = "#3366CC", "BT" = "#11B54E")) +
  theme(axis.text.x = element_text(angle = 0, hjust = 0.5))

```

```{r statistical testing}
# Fit the model using lmerTest to get p-values
model_competence <- lmerTest::lmer(Rating ~ Survey_Type + (1 | ID), data = filter(rosas_data, Measure == "Competence"))
summary(model_competence)

# Conduct Wilcoxon Signed Rank test for Warmth (non-normally distributed)
warmth_wilcox_test <- wilcox.test(survey_data_Base$`Warmth (Average)`, 
                                  survey_data_BT$`Warmth (Average)`, 
                                  paired = TRUE)

# Conduct Wilcoxon Signed Rank test for Discomfort (non-normally distributed)
discomfort_wilcox_test <- wilcox.test(survey_data_Base$`Discomfort (Average)`, 
                                      survey_data_BT$`Discomfort (Average)`, 
                                      paired = TRUE)

# Output the test results
list(
  Warmth_Wilcoxon_test = warmth_wilcox_test,
  Discomfort_Wilcoxon_test = discomfort_wilcox_test
)

```

###Statistical Test Results for RoSAS Measures **Competence** Test: Mixed-effects model with random intercept (participant-level)

Intercept Estimate: 4.756 (represents the average Competence rating for the Baseline condition) Survey_Type (BT) Estimate: 0.268 (difference in Competence rating for BT compared to Baseline) t-value: 1.68 p-value: 0.101 (not statistically significant)

**Interpretation**: The t-value of 1.68 and p-value of 0.101 indicate that the difference in Competence ratings between the baseline and BT conditions is not statistically significant at the 0.05 level. This suggests that there is insufficient evidence to conclude that the BT system significantly impacts perceptions of competence compared to the baseline condition, but might still be considered suggestive of a trend toward higher competence ratings for BT relative to Baseline.

**Warmth** Test: Wilcoxon Signed Rank Test with continuity correction V: 263.5 p-value: 0.2783

**Interpretation**: The p-value (0.2783) is above the 0.05 significance level, indicating no statistically significant difference in Warmth ratings between the baseline and BT conditions. This suggests that participants did not perceive a meaningful difference in warmth between the two systems.

**Discomfort** Test: Wilcoxon Signed Rank Test with continuity correction V: 483.5 p-value: 0.0184

**Interpretation**: The p-value (0.0184) is below the 0.05 significance threshold, indicating a statistically significant difference in Discomfort ratings between the baseline and BT conditions. This suggests that the BT system led to a significantly lower perception of discomfort compared to the baseline, supporting the hypothesis that the BT framework reduces discomfort for participants.

## H3: Trust (MDMT) and Explainability (xAI) Scales

-   **H3**: The BT Framework will achieve better results in both the MDMT (trust dimensions) and xAI (explainability) scales compared to the baseline system.
    -   *Reasoning*: The BT framework’s structured approach not only improves transparency and explainability but also positively impacts participants’ perceptions of the robot’s reliability, capability, and ethicality, as measured by the MDMT subscales. This holistic improvement supports both trust and explainability metrics.

### Visualization and Analysis

#### Data Preparation

```{r MDMT_xAI_Visualization}
# Convert columns with "Div by 0" to NA, ensuring exact column names
survey_data_Base <- survey_data_Base %>%
  mutate(across(c(`CAPACITY TRUST Average`, `Reliable Subscale Average`, 
                  `Capable Subscale Average`, `MORAL TRUST Average`, 
                  `Ethical Subscale Average`, `Sincere Subscale Average`, 
                  `Explainability Average`), 
                ~ as.numeric(replace(.x, .x == "Div by 0", NA))))

survey_data_BT <- survey_data_BT %>%
  mutate(across(c(`CAPACITY TRUST Average`, `Reliable Subscale Average`, 
                  `Capable Subscale Average`, `MORAL TRUST Average`, 
                  `Ethical Subscale Average`, `Sincere Subscale Average`, 
                  `Explainability Average`), 
                ~ as.numeric(replace(.x, .x == "Div by 0", NA))))

# Now calculate means for each scale, omitting NAs
mdmt_data <- data.frame(
  Measure = c("Capacity Trust", "Reliable Subscale", "Capable Subscale", 
              "Moral Trust", "Ethical Subscale", "Sincere Subscale", "Explainability"),
  Base = c(mean(survey_data_Base$`CAPACITY TRUST Average`, na.rm = TRUE),
           mean(survey_data_Base$`Reliable Subscale Average`, na.rm = TRUE),
           mean(survey_data_Base$`Capable Subscale Average`, na.rm = TRUE),
           mean(survey_data_Base$`MORAL TRUST Average`, na.rm = TRUE),
           mean(survey_data_Base$`Ethical Subscale Average`, na.rm = TRUE),
           mean(survey_data_Base$`Sincere Subscale Average`, na.rm = TRUE),
           mean(survey_data_Base$`Explainability Average`, na.rm = TRUE)),
  BT = c(mean(survey_data_BT$`CAPACITY TRUST Average`, na.rm = TRUE),
         mean(survey_data_BT$`Reliable Subscale Average`, na.rm = TRUE),
         mean(survey_data_BT$`Capable Subscale Average`, na.rm = TRUE),
         mean(survey_data_BT$`MORAL TRUST Average`, na.rm = TRUE),
         mean(survey_data_BT$`Ethical Subscale Average`, na.rm = TRUE),
         mean(survey_data_BT$`Sincere Subscale Average`, na.rm = TRUE),
         mean(survey_data_BT$`Explainability Average`, na.rm = TRUE))
)

# Reshape for visualization
mdmt_data_long <- mdmt_data %>%
  pivot_longer(cols = c(Base, BT), names_to = "Survey_Type", values_to = "Average")

ggplot(mdmt_data_long, aes(x = Measure, y = Average, fill = Survey_Type)) +
  geom_bar(stat = "identity", position = position_dodge(width = 0.8), color = "white") +
  labs(title = "Comparison of MDMT Trust Dimensions and Explainability (Average)",
       x = "Measure",
       y = "Average Rating",
       fill = "Survey Type") +
  theme_minimal() +
  scale_fill_manual(values = c("Base" = "#0C83C4", "BT" = "#11B54E")) + 
  geom_text(aes(label = round(Average, 2)), 
            position = position_dodge(width = 0.8), vjust = -0.25, size = 3.5) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

### Observations of MDMT Trust Dimensions and Explainability Ratings

The bar chart visualizes the average ratings for the MDMT trust dimensions (Capacity Trust, Moral Trust) and Explainability across the baseline and BT frameworks.

Higher Ratings for BT Framework: Across all dimensions, the BT Framework receives higher ratings than the baseline. This trend is especially pronounced in the Capable Subscale, where the BT Framework averages a score of 4.77 compared to the baseline's 4.18.

Explainability: The BT Framework also achieves a slightly higher average rating for explainability (4.55) compared to the baseline (4.37), suggesting participants found it more understandable or transparent.

Trust Dimensions: Capacity Trust: The BT Framework scores higher in Capacity Trust (4.8 vs. 4.3 for the baseline), with increases in both Reliable and Capable subscales. Moral Trust: There is a modest increase in Moral Trust for the BT Framework (4.34 vs. 4.07 for baseline), with notable gains in the Sincere Subscale (4.15 vs. 3.82). Overall, these results support the hypothesis that the BT Framework positively influences participants' perceptions of both trustworthiness and explainability.

```{r distributions visualized}
# Convert "Predictable MDMT" to numeric in both datasets, replacing non-numeric values with NA
survey_data_Base <- survey_data_Base %>%
  mutate(`Predictable MDMT` = as.numeric(replace(`Predictable MDMT`, !is.na(as.numeric(`Predictable MDMT`)), as.numeric(`Predictable MDMT`))))

survey_data_BT <- survey_data_BT %>%
  mutate(`Predictable MDMT` = as.numeric(replace(`Predictable MDMT`, !is.na(as.numeric(`Predictable MDMT`)), as.numeric(`Predictable MDMT`))))

# Combine datasets and assign Survey_Type
survey_data <- bind_rows(
  survey_data_Base %>% mutate(Survey_Type = "Base"),
  survey_data_BT %>% mutate(Survey_Type = "BT")
)


# Reshape data for plotting
mdmt_data_long <- survey_data %>%
  select(ParticipantID, Survey_Type, `CAPACITY TRUST Average`, `Reliable Subscale Average`, 
         `Capable Subscale Average`, `MORAL TRUST Average`, `Ethical Subscale Average`, 
         `Sincere Subscale Average`, `Explainability Average`) %>%
  pivot_longer(cols = starts_with(c("CAPACITY TRUST", "Reliable", "Capable", "MORAL TRUST", "Ethical", "Sincere", "Explainability")),
               names_to = "Measure", values_to = "Rating")

# Calculate summary statistics for plotting
stats_summary <- mdmt_data_long %>%
  group_by(Measure, Survey_Type) %>%
  summarise(Mean = mean(Rating, na.rm = TRUE), SD = sd(Rating, na.rm = TRUE), .groups = "drop")

# Plot for Capacity Trust, Reliable, and Capable
ggplot(mdmt_data_long %>% 
         filter(Measure %in% c("CAPACITY TRUST Average", "Reliable Subscale Average", "Capable Subscale Average")), 
       aes(x = Rating, fill = Survey_Type)) +
  geom_density(alpha = 0.4) +
  geom_vline(data = stats_summary %>% filter(Measure %in% c("CAPACITY TRUST Average", "Reliable Subscale Average", "Capable Subscale Average")),
             aes(xintercept = Mean, color = Survey_Type), linetype = "dashed", size = 1) +
  facet_wrap(~ factor(Measure, levels = c("CAPACITY TRUST Average", "Reliable Subscale Average", "Capable Subscale Average")), 
             scales = "free", ncol = 1) +
  labs(title = "MDMT Trust Dimensions: Capacity Trust, Reliable, and Capable",
       x = "Rating", y = "Density") +
  theme_minimal() +
  scale_fill_manual(values = c("Base" = "#3366CC", "BT" = "#11B54E")) +
  scale_color_manual(values = c("Base" = "#3366CC", "BT" = "#11B54E")) +
  theme(axis.text.x = element_text(angle = 0, hjust = 0.5))

# Plot for Moral Trust, Ethical, and Sincere
ggplot(mdmt_data_long %>% 
         filter(Measure %in% c("MORAL TRUST Average", "Ethical Subscale Average", "Sincere Subscale Average")), 
       aes(x = Rating, fill = Survey_Type)) +
  geom_density(alpha = 0.4) +
  geom_vline(data = stats_summary %>% filter(Measure %in% c("MORAL TRUST Average", "Ethical Subscale Average", "Sincere Subscale Average")),
             aes(xintercept = Mean, color = Survey_Type), linetype = "dashed", size = 1) +
  facet_wrap(~ factor(Measure, levels = c("MORAL TRUST Average", "Ethical Subscale Average", "Sincere Subscale Average")), 
             scales = "free", ncol = 1) +
  labs(title = "MDMT Trust Dimensions: Moral Trust, Ethical, and Sincere",
       x = "Rating", y = "Density") +
  theme_minimal() +
  scale_fill_manual(values = c("Base" = "#3366CC", "BT" = "#11B54E")) +
  scale_color_manual(values = c("Base" = "#3366CC", "BT" = "#11B54E")) +
  theme(axis.text.x = element_text(angle = 0, hjust = 0.5))

# Plot for Explainability
ggplot(mdmt_data_long %>% filter(Measure == "Explainability Average"),
       aes(x = Rating, fill = Survey_Type)) +
  geom_density(alpha = 0.4) +
  geom_vline(data = stats_summary %>% filter(Measure == "Explainability Average"),
             aes(xintercept = Mean, color = Survey_Type), linetype = "dashed", size = 1) +
  facet_wrap(~ factor(Measure, levels = c("Explainability Average")), 
             scales = "free", ncol = 1) +
  labs(title = "MDMT Trust Dimension: Explainability",
       x = "Rating", y = "Density") +
  theme_minimal() +
  scale_fill_manual(values = c("Base" = "#3366CC", "BT" = "#11B54E")) +
  scale_color_manual(values = c("Base" = "#3366CC", "BT" = "#11B54E")) +
  theme(axis.text.x = element_text(angle = 0, hjust = 0.5))
```

### Observations

**Plot 1**: Capacity Trust, Reliable Subscale, and Capable Subscale Capacity Trust: The BT system shows a slightly higher concentration around the higher end of the scale compared to the baseline system. This suggests that participants rated the BT system as more trustworthy in terms of capacity, with a higher average score than the baseline.

Reliable Subscale: Ratings for the BT system are again skewed slightly higher, showing a higher density around the 5-6 range compared to the baseline, which has a more balanced distribution. This indicates that the BT system is perceived as more reliable on average.

Capable Subscale: The BT system has a notable peak at a higher rating range than the baseline, indicating that participants find the BT system more capable. The baseline distribution, while similar in shape, peaks at a slightly lower rating, reinforcing the trend toward higher ratings for the BT system in perceived capability.

**Plot 2**: Moral Trust, Ethical Subscale, and Sincere Subscale Moral Trust: The BT system demonstrates a shift towards higher ratings in moral trust, with a density peak at the upper end of the scale. The baseline system, in comparison, has a broader spread and a lower density peak, indicating a weaker perception of moral trustworthiness.

Ethical Subscale: The BT system has a more concentrated distribution towards higher ratings, suggesting that participants rated it as more ethical than the baseline. The baseline’s distribution is wider, showing a more varied perception of ethicality across participants.

Sincere Subscale: The BT system’s ratings cluster more tightly around the higher end, suggesting a greater perception of sincerity compared to the baseline, which has a lower and broader spread.

**Plot 3**: Explainability Explainability: Both the baseline and BT system distributions show a bimodal pattern, indicating two distinct groups of ratings. This suggests that participants had split perceptions, with clusters of ratings in both lower and higher ranges. Despite this bimodal nature, the BT system's distribution skews more toward higher ratings, with one peak around 5-6, compared to the baseline, which has a broader spread and peaks at slightly lower values. This shift suggests that, while opinions were somewhat divided, the BT system was generally perceived as more explainable than the baseline.

```{r Normality Test}
# Shapiro-Wilk test for MDMT and xAI scales in Base and BT

# Capacity Trust
shapiro_capacity_base <- shapiro.test(survey_data_Base$`CAPACITY TRUST Average`)
shapiro_capacity_bt <- shapiro.test(survey_data_BT$`CAPACITY TRUST Average`)

# Reliable Subscale
shapiro_reliable_base <- shapiro.test(survey_data_Base$`Reliable Subscale Average`)
shapiro_reliable_bt <- shapiro.test(survey_data_BT$`Reliable Subscale Average`)

# Capable Subscale
shapiro_capable_base <- shapiro.test(survey_data_Base$`Capable Subscale Average`)
shapiro_capable_bt <- shapiro.test(survey_data_BT$`Capable Subscale Average`)

# Moral Trust
shapiro_moral_base <- shapiro.test(survey_data_Base$`MORAL TRUST Average`)
shapiro_moral_bt <- shapiro.test(survey_data_BT$`MORAL TRUST Average`)

# Ethical Subscale
shapiro_ethical_base <- shapiro.test(survey_data_Base$`Ethical Subscale Average`)
shapiro_ethical_bt <- shapiro.test(survey_data_BT$`Ethical Subscale Average`)

# Sincere Subscale
shapiro_sincere_base <- shapiro.test(survey_data_Base$`Sincere Subscale Average`)
shapiro_sincere_bt <- shapiro.test(survey_data_BT$`Sincere Subscale Average`)

# Explainability
shapiro_explainability_base <- shapiro.test(survey_data_Base$`Explainability Average`)
shapiro_explainability_bt <- shapiro.test(survey_data_BT$`Explainability Average`)

# Output the Shapiro-Wilk results
list(
  Capacity_Base = shapiro_capacity_base,
  Capacity_BT = shapiro_capacity_bt,
  Reliable_Base = shapiro_reliable_base,
  Reliable_BT = shapiro_reliable_bt,
  Capable_Base = shapiro_capable_base,
  Capable_BT = shapiro_capable_bt,
  Moral_Base = shapiro_moral_base,
  Moral_BT = shapiro_moral_bt,
  Ethical_Base = shapiro_ethical_base,
  Ethical_BT = shapiro_ethical_bt,
  Sincere_Base = shapiro_sincere_base,
  Sincere_BT = shapiro_sincere_bt,
  Explainability_Base = shapiro_explainability_base,
  Explainability_BT = shapiro_explainability_bt
)


```

| Measure           | Condition | W       | p-value   | Normally Distributed? |
|-------------------|-----------|---------|-----------|-----------------------|
| Capacity Trust    | Base      | 0.98586 | 0.8822    | Yes                   |
| Capacity Trust    | BT        | 0.93028 | 0.01475   | No                    |
| Reliable Subscale | Base      | 0.97727 | 0.5738    | Yes                   |
| Reliable Subscale | BT        | 0.91367 | 0.004309  | No                    |
| Capable Subscale  | Base      | 0.97145 | 0.3841    | Yes                   |
| Capable Subscale  | BT        | 0.93021 | 0.01467   | No                    |
| Moral Trust       | Base      | 0.97367 | 0.4665    | Yes                   |
| Moral Trust       | BT        | 0.96248 | 0.2034    | Yes                   |
| Ethical Subscale  | Base      | 0.97869 | 0.656     | Yes                   |
| Ethical Subscale  | BT        | 0.94684 | 0.0701    | Yes                   |
| Sincere Subscale  | Base      | 0.95641 | 0.1448    | Yes                   |
| Sincere Subscale  | BT        | 0.95666 | 0.1378    | Yes                   |
| Explainability    | Base      | 0.89725 | 0.001377  | No                    |
| Explainability    | BT        | 0.85383 | 9.319e-05 | No                    |

### Observation 
Only the Moral Trust scale and its two subscales are all normally distributed 
For Capacity trust and its subscales its always the Base being normally distributed but BT isn't
And in explainability both are not normally distributed.

```{r Statistical Analysis}
# Perform Wilcoxon Signed Rank Tests for Capacity Trust, Reliable Subscale, Capable Subscale, and Explainability

# Wilcoxon Signed Rank Test for Capacity Trust
wilcox_capacity <- wilcox.test(survey_data_Base$`CAPACITY TRUST Average`, 
                               survey_data_BT$`CAPACITY TRUST Average`, 
                               paired = TRUE, 
                               alternative = "two.sided")

# Wilcoxon Signed Rank Test for Reliable Subscale
wilcox_reliable <- wilcox.test(survey_data_Base$`Reliable Subscale Average`, 
                               survey_data_BT$`Reliable Subscale Average`, 
                               paired = TRUE, 
                               alternative = "two.sided")

# Wilcoxon Signed Rank Test for Capable Subscale
wilcox_capable <- wilcox.test(survey_data_Base$`Capable Subscale Average`, 
                              survey_data_BT$`Capable Subscale Average`, 
                              paired = TRUE, 
                              alternative = "two.sided")

# Wilcoxon Signed Rank Test for Explainability
wilcox_explainability <- wilcox.test(survey_data_Base$`Explainability Average`, 
                                     survey_data_BT$`Explainability Average`, 
                                     paired = TRUE, 
                                     alternative = "two.sided")

# Output the results
list(
  Capacity_Trust = wilcox_capacity,
  Reliable_Subscale = wilcox_reliable,
  Capable_Subscale = wilcox_capable,
  Explainability = wilcox_explainability
)

```
In short:
Significant differences were found for Capacity Trust and its subscales (Reliable and Capable).
No significant difference was found for Explainability.


```{r Statistical analysis}
# Paired t-tests for Moral Trust and its subscales

# Moral Trust Average
t_test_moral <- t.test(survey_data_Base$`MORAL TRUST Average`, survey_data_BT$`MORAL TRUST Average`, 
                       paired = TRUE)

# Ethical Subscale Average
t_test_ethical <- t.test(survey_data_Base$`Ethical Subscale Average`, survey_data_BT$`Ethical Subscale Average`, 
                         paired = TRUE)

# Sincere Subscale Average
t_test_sincere <- t.test(survey_data_Base$`Sincere Subscale Average`, survey_data_BT$`Sincere Subscale Average`, 
                         paired = TRUE)

# Output the results
list(
  Moral_Trust = t_test_moral,
  Ethical_Subscale = t_test_ethical,
  Sincere_Subscale = t_test_sincere
)

```

Only the Sincere subscale shows a statistically significant difference between the two conditions, suggesting that participants rated the BT framework differently from the Baseline in terms of sincerity. For the other two measures (Moral Trust and Ethical Subscale), the differences are not statistically significant, meaning we do not have enough evidence to say there’s a reliable difference between Baseline and BT for those scales.

This means that within the Moral Trust dimension, only sincerity has a measurable difference, indicating that participants found the BT condition significantly different (presumably more sincere) than the Baseline condition.

```{r Statistical analysis mixed effect models MDMT}
# Prepare the data for mixed-effects model
mdmt_data_long <- survey_data %>%
  select(ParticipantID, Survey_Type, `MORAL TRUST Average`, 
         `Ethical Subscale Average`, `Sincere Subscale Average`) %>%
  pivot_longer(cols = c(`MORAL TRUST Average`, `Ethical Subscale Average`, `Sincere Subscale Average`),
               names_to = "Measure", values_to = "Rating")

# Fit mixed-effects models for each measure
model_moral <- lmer(Rating ~ Survey_Type + (1 | ParticipantID), 
                    data = filter(mdmt_data_long, Measure == "MORAL TRUST Average"))

model_ethical <- lmer(Rating ~ Survey_Type + (1 | ParticipantID), 
                      data = filter(mdmt_data_long, Measure == "Ethical Subscale Average"))

model_sincere <- lmer(Rating ~ Survey_Type + (1 | ParticipantID), 
                      data = filter(mdmt_data_long, Measure == "Sincere Subscale Average"))

# Summarize the models
summary(model_moral)
summary(model_ethical)
summary(model_sincere)
```

Observation of Mixed-Effects Model Results
Moral Trust Scale:

The fixed effect for Survey_Type (BT) has an estimate of 0.328 with a p-value of 0.072.
This result suggests a potential trend toward higher ratings for the BT system, but it is not statistically significant (p > 0.05).
Ethical Subscale:

The fixed effect for Survey_Type (BT) has an estimate of 0.240 with a p-value of 0.243.
There is no statistically significant difference between the Baseline and BT systems for this subscale.
Sincere Subscale:

The fixed effect for Survey_Type (BT) has an estimate of 0.396 with a p-value of 0.047.
This indicates a statistically significant difference, with the BT system receiving higher ratings compared to the Baseline system.
Summary: The results from the mixed-effects model align with previous findings:

Only the Sincere Subscale shows a statistically significant improvement for the BT system.
The Moral Trust Scale and Ethical Subscale show no significant differences.

## H4: Participant Preference for BT-LLM Framework

-   **H4**: Participants will prefer the BT-LLM framework over the baseline system.

### Visualization and Analysis

#### Data Preparation

```{r Preferred System}
# Calculate the frequency of each preferred system
preferred_data <- data %>%
  count(Preferred_System = `Preferred System`) %>%
  mutate(Percentage = n / sum(n) * 100)

# Create a pie chart for preferred system distribution
ggplot(preferred_data, aes(x = "", y = Percentage, fill = Preferred_System)) +
  geom_bar(stat = "identity", width = 1, color = "white") +
  coord_polar(theta = "y") +
  labs(title = "Distribution of Preferred Systems",
       x = NULL,
       y = NULL,
       fill = "Preferred System") +
  theme_void() +
  geom_text(aes(label = paste0(round(Percentage, 1), "%")), 
            position = position_stack(vjust = 0.5))+
  scale_fill_manual(values = c("BT" = "#11B54E", "Base" = "#0C83C4")) # Customize with darker blue and red

```

seems like they prefer bt

statistical tests with assumption there is no preference 50/50 distribution will be made

```{r}
# Perform Binomial Test
total_responses <- sum(preferred_data$n)
bt_count <- preferred_data$n[preferred_data$Preferred_System == "BT"]
binom_test <- binom.test(x = bt_count, n = total_responses, p = 0.5, alternative = "greater")
binom_test

```


Interpretation of the Results
Number of Successes = 30, Number of Trials = 41:

This indicates that out of 41 participants, 30 chose the BT framework.
p-value = 0.002162:

The p-value here is quite low (0.0022), which is below the conventional significance level of 0.05.
This indicates that the observed preference for the BT framework is statistically significant, meaning it is unlikely to have occurred by chance if there were no real preference among participants.
Alternative Hypothesis:

The test was one-sided, with the alternative hypothesis being that the true probability of preferring the BT framework is greater than 0.5 (50%).
Since the p-value is low, we reject the null hypothesis (that preference is 50%) and support the alternative hypothesis, suggesting a real preference for the BT system over the baseline.
95% Confidence Interval (0.5952 to 1.0000):

This confidence interval represents the range within which we expect the true proportion of participants preferring the BT framework to lie, with 95% confidence.
The interval suggests that between 59.5% and 100% of participants would prefer the BT framework over the baseline in the population, supporting the idea that there is a strong preference for BT.
Probability of Success (0.7317):

This is the sample estimate of the proportion of participants who preferred the BT framework, calculated as 30 out of 41, or about 73.2%.
Summary
The binomial test shows strong evidence that participants significantly preferred the BT framework, with an estimated 73.2% preferring it over the baseline. This preference is statistically significant, with a p-value of 0.0022, and suggests that the BT framework was generally more favored by participants than the baseline system.


## Demographics

```{r Demographics optics}
# Gender Pie Chart
gender_data <- data %>%
  count(Gender = `Gender`) %>%
  mutate(Percentage = n / sum(n) * 100)

gender_plot <- ggplot(gender_data, aes(x = "", y = Percentage, fill = Gender)) +
  geom_bar(stat = "identity", width = 1, color = "white") +
  coord_polar(theta = "y") +
  labs(title = "Gender Distribution",
       x = NULL,
       y = NULL,
       fill = "Gender") +
  theme_void() +
  geom_text(aes(label = paste0(round(Percentage, 1), "%")), 
            position = position_stack(vjust = 0.5)) +
  scale_fill_manual(values = c("Female" = "#33adbb", "Male" = "#fd7e23", 
                               "Non-binary" = "#34495E", 
                               "Prefer not to say" = "#7D3C98"))

# Display Gender Plot
print(gender_plot)

# Age Histogram
average_age <- mean(data$Age, na.rm = TRUE)

age_plot <- ggplot(data, aes(x = Age)) +
  geom_histogram(binwidth = 1, fill = "#3366CC", color = "white", alpha = 0.7) +
  labs(title = "Age Distribution",
       x = "Age",
       y = "Frequency") +
  theme_minimal() +
  geom_vline(aes(xintercept = average_age), color = "red", linetype = "dashed", size = 1) +
  annotate("text", x = average_age + 1, y = max(table(data$Age)), 
           label = paste("Avg Age:", round(average_age, 1)), color = "black", vjust = 1, hjust = 0)

# Display Age Plot
print(age_plot)

# English Proficiency Bar Plot in Cyan
english_data <- data %>%
  count(Proficiency = `English Proficiency`) %>%
  mutate(Proficiency = factor(Proficiency, levels = c("Native or bilingual proficiency", 
                                                      "Advanced proficiency", 
                                                      "Intermediate proficiency", 
                                                      "Basic proficiency")))

english_plot <- ggplot(english_data, aes(x = Proficiency, y = n)) +
  geom_bar(stat = "identity", fill = "#6DAF81", color = "black", alpha = 0.8) + # Soft cyan
  labs(title = "English Proficiency",
       x = "Proficiency Level",
       y = "Count") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Display English Proficiency Plot
print(english_plot)

# Qualification Bar Plot in Purple
qualification_data <- data %>%
  count(Qualification = `Qualification`) %>%
  mutate(Qualification = fct_relevel(Qualification, 
                                     "Master's degree", 
                                     "Bachelor's degree", 
                                     "Some college, no degree", 
                                     "High school diploma or equivalent", 
                                     "No formal education"))

qualification_plot <- ggplot(qualification_data, aes(x = Qualification, y = n)) +
  geom_bar(stat = "identity", fill = "#A992C4", color = "black", alpha = 0.8) + # Soft purple
  labs(title = "Qualification Distribution",
       x = "Qualification Level",
       y = "Count") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Display Qualification Plot
print(qualification_plot)



```

```{r }
# Gender Pie Chart
gender_plot <- ggplot(gender_data, aes(x = "", y = Percentage, fill = Gender)) +
  geom_bar(stat = "identity", width = 1, color = "white") +
  coord_polar(theta = "y") +
  labs(title = "Gender Distribution",
       x = NULL,
       y = NULL,
       fill = "Gender") +
  theme_void() +
  geom_text(aes(label = paste0(round(Percentage, 1), "%")), 
            position = position_stack(vjust = 0.5), size = 5) + # Increase text size
  scale_fill_manual(values = c("Female" = "#33adbb", "Male" = "#fd7e23", 
                               "Non-binary" = "#34495E", 
                               "Prefer not to say" = "#7D3C98")) +
  theme(
    plot.title = element_text(size = 20),  # Title font size
    legend.title = element_text(size = 15),
    legend.text = element_text(size = 15)
  )

print(gender_plot)

# Age Histogram
age_plot <- ggplot(data, aes(x = Age)) +
  geom_histogram(binwidth = 1, fill = "#3366CC", color = "white", alpha = 0.7) +
  labs(title = "Age Distribution",
       x = "Age",
       y = "Frequency") +
  theme_minimal() +
  geom_vline(aes(xintercept = average_age), color = "red", linetype = "dashed", size = 1) +
  annotate("text", x = average_age + 1, y = max(table(data$Age)), 
           label = paste("Avg Age:", round(average_age, 1)), color = "black", vjust = 1, hjust = 0, size = 5) +
  theme(
    plot.title = element_text(size = 20),
    axis.title = element_text(size = 13),
    axis.text = element_text(size = 12)
  )

print(age_plot)

# English Proficiency Bar Plot in Cyan
english_plot <- ggplot(english_data, aes(x = Proficiency, y = n)) +
  geom_bar(stat = "identity", fill = "#6DAF81", color = "black", alpha = 0.8) +
  labs(title = "English Proficiency",
       x = "Proficiency Level",
       y = "Number of Participants") +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 20),
    axis.title = element_text(size = 13),
    axis.text.x = element_text(angle = 45, hjust = 1, size = 12),
    axis.text.y = element_text(size = 12)
  )

print(english_plot)

# Qualification Bar Plot in Purple
qualification_plot <- ggplot(qualification_data, aes(x = Qualification, y = n)) +
  geom_bar(stat = "identity", fill = "#A992C4", color = "black", alpha = 0.8) +
  labs(title = "Qualification Distribution",
       x = "Qualification Level",
       y = "Number of Participants") +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 20),
    axis.title = element_text(size = 13),
    axis.text.x = element_text(angle = 45, hjust = 1, size = 12),
    axis.text.y = element_text(size = 12)
  )

print(qualification_plot)

```
```{r statistics on the data}
# Calculate Gender Distribution
gender_summary <- data %>%
  count(Gender = `Gender`) %>%
  mutate(Percentage = round(n / sum(n) * 100, 1))
print("Gender Distribution:")
print(gender_summary)

# Calculate Age Statistics
age_summary <- data %>%
  summarise(
    Mean_Age = round(mean(Age, na.rm = TRUE), 1),
    Median_Age = round(median(Age, na.rm = TRUE), 1),
    Min_Age = min(Age, na.rm = TRUE),
    Max_Age = max(Age, na.rm = TRUE),
    Age_SD = round(sd(Age, na.rm = TRUE), 1)
  )
print("Age Statistics:")
print(age_summary)

# Calculate Qualification Distribution
qualification_summary <- data %>%
  count(Qualification = `Qualification`) %>%
  mutate(Percentage = round(n / sum(n) * 100, 1))
print("Qualification Distribution:")
print(qualification_summary)

# Calculate English Proficiency Distribution
proficiency_summary <- data %>%
  count(Proficiency = `English Proficiency`) %>%
  mutate(Percentage = round(n / sum(n) * 100, 1))
print("English Proficiency Distribution:")
print(proficiency_summary)

# Combine all summaries into a list for easier inspection
demographics_summary <- list(
  Gender_Distribution = gender_summary,
  Age_Statistics = age_summary,
  Qualification_Distribution = qualification_summary,
  English_Proficiency_Distribution = proficiency_summary
)

# Display all summaries
print("Demographics Summary:")
print(demographics_summary)



```

## Exploratoty Analysis

### Users who spent less time with BT than with Base

```{r Visualization }
# Convert time columns to seconds for comparison
data <- data %>%
  mutate(
    Time_Baseline_Seconds = as.numeric(sub(":.*", "", `Time spent with Baseline in mm:ss`)) * 60 +
                              as.numeric(sub(".*:", "", `Time spent with Baseline in mm:ss`)),
    Time_BT_Seconds = as.numeric(sub(":.*", "", `Time spent with BT in mm:ss`)) * 60 +
                      as.numeric(sub(".*:", "", `Time spent with BT in mm:ss`))
  )

# Filter cases where time spent with BT is less than Baseline
faster_with_BT <- data %>%
  filter(Time_BT_Seconds < Time_Baseline_Seconds)

# Display participant IDs where BT was faster
faster_participant_ids <- faster_with_BT %>%
  select(`ParticipantID`, `Preferred System`, Time_Baseline_Seconds, Time_BT_Seconds)

print(faster_participant_ids)

# Count preferences for these cases
preferences <- faster_with_BT %>%
  count(`Preferred System`)

# Create the barplot with improved aesthetics
ggplot(preferences, aes(x = `Preferred System`, y = n, fill = `Preferred System`)) +
  geom_bar(stat = "identity", color = "black", alpha = 0.8, width = 0.7) +
  labs(
    title = "Preferred System When BT Was Faster",
    x = "Preferred System",
    y = "Number of Participants"
  ) +
  theme_minimal() +
  scale_fill_manual(values = c("BT" = "#11B54E", "Base" = "#0C83C4")) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
    axis.title.x = element_text(size = 12),
    axis.title.y = element_text(size = 12)
  )


```
### Correlation faster system and preferred system

```{r correlation}
# Calculate the time difference and total time
data <- data %>%
  mutate(
    Time_Baseline_Seconds = as.numeric(sub(":.*", "", `Time spent with Baseline in mm:ss`)) * 60 +
                              as.numeric(sub(".*:", "", `Time spent with Baseline in mm:ss`)),
    Time_BT_Seconds = as.numeric(sub(":.*", "", `Time spent with BT in mm:ss`)) * 60 +
                      as.numeric(sub(".*:", "", `Time spent with BT in mm:ss`)),
    Time_Difference = Time_Baseline_Seconds - Time_BT_Seconds,
    Total_Time = Time_Baseline_Seconds + Time_BT_Seconds
  )

# Scatter plot showing correlation between time difference and preferred system
ggplot(data, aes(x = Time_Difference, y = Total_Time, color = `Preferred System`)) +
  geom_point(size = 4, alpha = 0.8) +
  geom_smooth(method = "lm", se = FALSE, linetype = "dashed", color = "black") +
  labs(
    title = "Correlation Between Time Difference and Preferred System",
    x = "Time Difference (Baseline - BT) [seconds]",
    y = "Total Time Spent [seconds]",
    color = "Preferred System"
  ) +
  theme_minimal() +
  scale_color_manual(values = c("BT" = "#11B54E", "Base" = "#0C83C4")) +
  theme(
    plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
    axis.title.x = element_text(size = 12),
    axis.title.y = element_text(size = 12)
  )

```


```{r}
# Determine which system was faster
data <- data %>%
  mutate(
    Faster_System = ifelse(Time_Baseline_Seconds < Time_BT_Seconds, "Baseline", "BT")
  )

# Create a cross-tabulation of Faster System and Preferred System
faster_vs_preferred <- table(data$Faster_System, data$`Preferred System`)

# Perform a Chi-Square test to check for association
chi_square_test <- chisq.test(faster_vs_preferred)

# Print the cross-tabulation and test results
print(faster_vs_preferred)
print(chi_square_test)

# Bar Plot to visualize the relationship
ggplot(data, aes(x = Faster_System, fill = `Preferred System`)) +
  geom_bar(position = "dodge", color = "black") +
  labs(
    title = "Faster System vs Preferred System",
    x = "Faster System",
    y = "Count",
    fill = "Preferred System"
  ) +
  theme_minimal() +
  scale_fill_manual(values = c("BT" = "#11B54E", "Base" = "#0C83C4")) +
  theme(
    plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
    axis.title.x = element_text(size = 12),
    axis.title.y = element_text(size = 12)
  )

```

### Reliability Analysis

**H2**
```{r cornbach alpha H2}
# Combine relevant columns into a data frame for Baseline and BT
rosas_baseline <- survey_data_Base[, c("Warmth (Average)", "Competence (Average)", "Discomfort (Average)")]
rosas_bt <- survey_data_BT[, c("Warmth (Average)", "Competence (Average)", "Discomfort (Average)")]

# Remove rows with missing values
rosas_baseline <- na.omit(rosas_baseline)
rosas_bt <- na.omit(rosas_bt)

# Compute Cronbach's alpha for Baseline
alpha_baseline_rosas <- psych::alpha(rosas_baseline, check.keys = TRUE)

# Compute Cronbach's alpha for BT
alpha_bt_rosas <- psych::alpha(rosas_bt, check.keys = TRUE)

# Display results
alpha_baseline_rosas
alpha_bt_rosas

```

#### Summary
**Warmth:**
Baseline Cronbach's Alpha: 0.53 (poor).
BT Cronbach's Alpha: 0.49 (poor).
Interpretation: The items do not cohesively measure the concept of "Warmth" in either condition.

**Competence:**
Baseline Cronbach's Alpha: 0.42 (poor).
BT Cronbach's Alpha: 0.25 (very poor).
Interpretation: Items are not well-designed for assessing "Competence."

**Discomfort:**
Baseline Cronbach's Alpha: 0.76 (good).
BT Cronbach's Alpha: 0.56 (moderate).
Interpretation: The "Discomfort" scale shows the strongest internal consistency but still has room for improvement in the BT condition.

**Overall:**
The results suggest potential issues with item design for Warmth and Competence in both baseline and BT conditions.
Discomfort demonstrates the strongest consistency, indicating better scale reliability


```{r H3 cornbach alpha}
# Combine relevant columns into a data frame for Baseline and BT
mdmt_xai_baseline <- survey_data_Base[, c("CAPACITY TRUST Average", "Reliable Subscale Average", 
                                          "Capable Subscale Average", "MORAL TRUST Average", 
                                          "Ethical Subscale Average", "Sincere Subscale Average", 
                                          "Explainability Average")]

mdmt_xai_bt <- survey_data_BT[, c("CAPACITY TRUST Average", "Reliable Subscale Average", 
                                  "Capable Subscale Average", "MORAL TRUST Average", 
                                  "Ethical Subscale Average", "Sincere Subscale Average", 
                                  "Explainability Average")]

# Remove rows with missing values
mdmt_xai_baseline <- na.omit(mdmt_xai_baseline)
mdmt_xai_bt <- na.omit(mdmt_xai_bt)

# Compute Cronbach's alpha for Baseline
alpha_baseline_mdmt <- psych::alpha(mdmt_xai_baseline)

# Compute Cronbach's alpha for BT
alpha_bt_mdmt <- psych::alpha(mdmt_xai_bt)

# Display results
alpha_baseline_mdmt
alpha_bt_mdmt

```

#### Summary
**Capacity Trust:**
Baseline Cronbach's Alpha: 0.79 (acceptable).
BT Cronbach's Alpha: 0.80 (acceptable).
Interpretation: Items demonstrate moderate internal consistency for measuring Capacity Trust in both conditions.

**Reliable Subscale:**
Baseline Cronbach's Alpha: 0.79 (acceptable).
BT Cronbach's Alpha: 0.80 (acceptable).
Interpretation: Items reliably assess the concept of reliability within Capacity Trust.

**Capable Subscale:**
Baseline Cronbach's Alpha: 0.81 (good).
BT Cronbach's Alpha: 0.81 (good).
Interpretation: Strong internal consistency, indicating that this subscale effectively measures participants' perceptions of capability.

**Moral Trust:**
Baseline Cronbach's Alpha: 0.78 (acceptable).
BT Cronbach's Alpha: 0.78 (acceptable).
Interpretation: Moderate reliability in assessing Moral Trust across both systems.

**Ethical Subscale:**
Baseline Cronbach's Alpha: 0.80 (good).
BT Cronbach's Alpha: 0.80 (good).
Interpretation: Good consistency for measuring ethical perceptions.

**Sincere Subscale:**
Baseline Cronbach's Alpha: 0.79 (acceptable).
BT Cronbach's Alpha: 0.79 (acceptable).
Interpretation: Acceptable reliability for evaluating perceptions of sincerity.

**Explainability:**
Baseline Cronbach's Alpha: 0.95 (excellent).
BT Cronbach's Alpha: 0.93 (excellent).
Interpretation: Exceptional internal consistency, confirming that Explainability is measured reliably in both conditions.

**Overall:**
Subscales demonstrate acceptable to good internal consistency, with the strongest results in Explainability.
Both Baseline and BT frameworks produce reliable data for evaluating perceptions of trust and explainability.